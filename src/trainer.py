# trainer.py (Phi√™n b·∫£n n√¢ng c·∫•p v·ªõi Data Balancing v√† Target th·ª±c t·∫ø)
import logging
import sqlite3
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report # C·∫¨P NH·∫¨T: Th√™m th∆∞ vi·ªán ƒë·ªÉ b√°o c√°o chi ti·∫øt
import joblib
from . import config as config

logger = logging.getLogger(__name__)

def train_model() -> float | None:
    """
    Hu·∫•n luy·ªán model d·ª±a tr√™n k·∫øt qu·∫£ WIN/LOSS th·ª±c t·∫ø v√† tr·∫£ v·ªÅ ƒë·ªô ch√≠nh x√°c (accuracy).
    S·ª≠ d·ª•ng k·ªπ thu·∫≠t c√¢n b·∫±ng d·ªØ li·ªáu (undersampling).
    Tr·∫£ v·ªÅ None n·∫øu c√≥ l·ªói ho·∫∑c kh√¥ng ƒë·ªß d·ªØ li·ªáu h·ª£p l·ªá.
    """
    logger.info("üöÄ Starting Advanced Model Training...")
    
    if df.empty:
        logger.warning("Training data is empty. Skipping training.")
        return None # Tr·∫£ v·ªÅ None ƒë·ªÉ b√°o hi·ªáu kh√¥ng hu·∫•n luy·ªán

    logger.info(f"Loaded {df.shape[0]} records for training.")
    logger.info("Data types and non-null counts:\n" + str(df.info()))
    
    # Thay 'outcome' b·∫±ng t√™n c·ªôt k·∫øt qu·∫£ th·ª±c t·∫ø c·ªßa b·∫°n (v√≠ d·ª•: 'signal', 'target')
    outcome_col = 'outcome' 
    if outcome_col in df.columns:
        logger.info(f"Value counts for '{outcome_col}' column:\n" + str(df[outcome_col].value_counts()))
        # Ki·ªÉm tra n·∫øu ch·ªâ c√≥ 1 lo·∫°i k·∫øt qu·∫£
        if df[outcome_col].nunique() < 2:
            logger.error(f"Training failed: Only one class ('{df[outcome_col].unique()[0]}') found in the outcome column. Cannot train.")
            return None
    else:
        logger.error(f"Training failed: The outcome column '{outcome_col}' was not found in the training data!")
        return None

    try:
        with sqlite3.connect(config.SQLITE_DB_PATH) as conn:
            # C·∫¢I TI·∫æN: L·∫•y t·∫•t c·∫£ c√°c giao d·ªãch ƒë√£ ƒë√≥ng (status != 'ACTIVE')
            # v√† bao g·ªìm c·∫£ pnl_percentage ƒë·ªÉ x√°c ƒë·ªãnh k·∫øt qu·∫£ m·ªôt c√°ch ch√≠nh x√°c.
            query = """
            SELECT 
                ema_fast_val, ema_medium_val, ema_slow_val, 
                rsi_val, atr_val, 
                bbands_lower, bbands_middle, bbands_upper, 
                trend,
                status,
                pnl_percentage
            FROM trend_analysis
            WHERE 
                status != 'ACTIVE' 
                AND pnl_percentage IS NOT NULL 
                AND trend IS NOT NULL
            """
            df = pd.read_sql(query, conn)
    except Exception as e:
        logger.error(f"‚ùå Failed to load data for training: {e}", exc_info=True)
        return None

    if df.empty:
        logger.warning("‚ö†Ô∏è No completed WIN/LOSS trades found. Skipping training.")
        return None

    # C·∫¢I TI·∫æN: T·∫°o c·ªôt 'outcome' m·ªôt c√°ch linh ho·∫°t trong Python.
    # M·ªôt giao d·ªãch l√† 'WIN' n·∫øu n√≥ ch·∫°m TP ho·∫∑c c√≥ PnL > 0.
    df['outcome'] = df.apply(
        lambda row: 'WIN' if ('TP' in row['status'] or row['pnl_percentage'] > 0) else 'LOSS',
        axis=1
    )
    # C·∫¨P NH·∫¨T: Danh s√°ch features ban ƒë·∫ßu, 'trend' gi·ªù l√† m·ªôt feature
    initial_features = [
        'ema_fast_val', 'ema_medium_val', 'ema_slow_val',
        'rsi_val', 'atr_val',
        'bbands_lower', 'bbands_middle', 'bbands_upper',
        'trend' 
    ]
    target = 'outcome' # C·∫¨P NH·∫¨T: M·ª•c ti√™u d·ª± ƒëo√°n l√† 'outcome'

    df.dropna(subset=initial_features + [target], inplace=True)

    if len(df) < 50: # ƒê·∫∑t m·ªôt ng∆∞·ª°ng t·ªëi thi·ªÉu cao h∆°n cho d·ªØ li·ªáu s·∫°ch
        logger.warning(f"‚ö†Ô∏è Not enough clean data. Only {len(df)} rows. Minimum 50 required.")
        return None

    # C·∫¨P NH·∫¨T: Logic c√¢n b·∫±ng d·ªØ li·ªáu (Undersampling)
    logger.info("‚öñÔ∏è Balancing data using Undersampling...")
    df_wins = df[df[target] == 'WIN']
    df_losses = df[df[target] == 'LOSS']

    if df_wins.empty or df_losses.empty:
        logger.warning(f"‚ö†Ô∏è Training data needs both WIN and LOSS samples. Skipping.")
        return None

    min_samples = min(len(df_wins), len(df_losses))
    logger.info(f"Balancing to {min_samples} WINs and {min_samples} LOSSes.")
    
    df_balanced = pd.concat([
        df_wins.sample(n=min_samples, random_state=42),
        df_losses.sample(n=min_samples, random_state=42)
    ])

    # C·∫¨P NH·∫¨T: X·ª≠ l√Ω feature 'trend' (d·∫°ng text) b·∫±ng One-Hot Encoding
    X_pre_process = df_balanced[initial_features]
    y = df_balanced[target]
    
    X = pd.get_dummies(X_pre_process, columns=['trend'], drop_first=True)
    
    # L∆∞u l·∫°i danh s√°ch features cu·ªëi c√πng sau khi x·ª≠ l√Ω
    final_features = X.columns.tolist()

    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # V√¨ ƒë√£ c√¢n b·∫±ng, stratify kh√¥ng c√≤n qu√° quan tr·ªçng nh∆∞ng v·∫´n n√™n gi·ªØ
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

    logger.info("ü§ñ Fitting RandomForestClassifier model...")
    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')
    model.fit(X_train, y_train)

    # L∆∞u model v√† c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt
    joblib.dump(model, "model_trend.pkl")
    joblib.dump(label_encoder, "trend_label_encoder.pkl")
    joblib.dump(final_features, "model_features.pkl") # C·∫¨P NH·∫¨T: L∆∞u danh s√°ch feature cu·ªëi c√πng

    accuracy = model.score(X_test, y_test)
    logger.info(f"‚úÖ Model trained successfully. Accuracy on test set: {accuracy:.2%}")

    # C·∫¨P NH·∫¨T: In b√°o c√°o chi ti·∫øt
    logger.info("üìä Classification Report on Test Set:")
    y_pred = model.predict(X_test)
    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)
    print(report) # In ra console ƒë·ªÉ xem
    logger.info(f"\n{report}")
    
    return accuracy
